{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie-Zhu1210/casa0018_workshop/blob/main/Week3/CASA0018_3_1_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnyTxjK_GbOD"
      },
      "source": [
        "# MNIST\n",
        "This example MNIST database. Information on the NMIST character dataset can be found [here](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "# Importing dependencies\n",
        "Let's start by importing TensorFlow, printing out the version number and create an object that points to the MNIST data via the tf.keras datasets API. How did we know where the dataset was? TensorFlow Datasets provide a list of resources that are easily accessible in the Colabs - a catalogue of what is availabe is at: https://www.tensorflow.org/datasets/catalog/overview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3KzJyjv3rnA",
        "outputId": "5aec9be3-2519-4486-83b6-a3400551b5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuoLQQBT4E-_"
      },
      "source": [
        "\n",
        "# Loading the image data\n",
        "Calling **load_data** on the **mnist** object will give you two sets of lists, these will be the training and testing values for the graphics that contain the clothing items and their labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BTdRgExe4TRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b93ab2-19ce-41cf-faac-889dde1de1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw395ROx4f5Q"
      },
      "source": [
        "# Exploring the data and preparing it for use\n",
        "What does these values look like? We are going to import a library that helps us display an image and point it to a training image, and a training label to see. Experiment with different indices in the array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "FPc9d3gJ3jWF",
        "outputId": "5c711c32-ecc8-4c80-d01e-e5bb58d6e858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (28, 28)\n",
            "Label: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sample = 6;\n",
        "print(\"shape:\", training_images[sample].shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[sample], cmap='gray')\n",
        "print(\"Label:\", training_labels[sample])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbrdH225_nH"
      },
      "source": [
        "Each of the 'pixel' values above are between 0 (black) and 255 (white).\n",
        "\n",
        "To '**normalize**' the data between 0 and 1, in Python we just need to divide all values by 255.0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kRH19pWs6ZDn"
      },
      "outputs": [],
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIn7S9gf62ie"
      },
      "source": [
        "# Building the model\n",
        "To create the sequence of layers in a model we use  '**Sequential**'. The model has 3 layers. First layer is called '**Flatten**' which takes the square image (28x28 pixels) and turns it into a 1 dimensional set. The second layer '**Dense**' has 24 neurons and the 3rd layer '**Dense**' has 10 neurons. Note the input to the model is a 28x28 image and the output is one of 10 neurons which relates to the 10 fashion categories.\n",
        "\n",
        "The activition functions are code that runs when the model is training - '**relu**' (Rectified Linear Unit - ReLU) converts any negative value to zero, '**softmax**' looks at all the probabilities in that layer of neurons and sets the highest value to 1 and all the others to 0 - this makes it programatically easier to find the most likely solution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "7mAyndG3kVlK",
        "outputId": "c78995e5-b971-41af-de9d-fad852d2c149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │        \u001b[38;5;34m18,840\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m250\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,840</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,090\u001b[0m (74.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,090</span> (74.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,090\u001b[0m (74.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,090</span> (74.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                                    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vbMCqb9Mh6"
      },
      "source": [
        "After defining the model we build it by compiling with an optimizer and loss function and then train it by calling **model.fit** to fit your training data to your training labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLMdl9aP8nQ0",
        "outputId": "b46b6cf7-a658-411e-ef05-f5f05de9448f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.6517\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9357 - loss: 0.2276\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9480 - loss: 0.1790\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1504\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1354\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b5a9271f5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJMsvSB-1UY"
      },
      "source": [
        "The accuracy value at the end of the final epoch is about 0.95 or about 95% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 95% of the time.\n",
        "\n",
        "Next we can call '**model.evaluate**', and pass in the test data, and it will report back the loss for each. Let's give it a try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzlqsEzX9s5P",
        "outputId": "56409331-6e3f-46d5-c0c9-4a66390d8649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1695\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1463928371667862, 0.9574000239372253]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tki-Aro_Uax"
      },
      "source": [
        "For me, that returned a accuracy of about .9579, which means it was about 95% accurate. As expected it probably would not do as well with *unseen* data as it did with data it was trained on!  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CASA0018-2-1-MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}